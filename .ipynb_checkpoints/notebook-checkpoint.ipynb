{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "#wandb.init(project=\"image-similarity-search\", entity=\"ajwadakil\")\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torchvision.transforms as T\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def handle_dataset():\n",
    "#     transform = torchvision.transforms.ToTensor()\n",
    "#     caltech256 = torchvision.datasets.Caltech256(root='./data', download=True, transform=transform)\n",
    "#\n",
    "#     # train_dataset, val_dataset = torch.utils.data.random_split(caltech256, [0.75, 0.25])\n",
    "#\n",
    "#     # # Create the train dataloader\n",
    "#     # train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#\n",
    "#     # # Create the validation dataloader\n",
    "#     # val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "#\n",
    "#     # # Create the full dataloader\n",
    "#     # full_loader = torch.utils.data.DataLoader(caltech256, batch_size=32, shuffle=True)\n",
    "#     # #data_loader = torch.utils.data.DataLoader(dataset=caltech256, batch_size=64, shuffle=True)\n",
    "#\n",
    "#     return train_loader, val_loader, full_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = './data/caltech256/256_ObjectCategories'\n",
    "# # get all the folder paths\n",
    "# all_paths = os.listdir(root_dir)\n",
    "#\n",
    "# # create a DataFrame\n",
    "# data = pd.DataFrame()\n",
    "#\n",
    "# images = []\n",
    "# labels = []\n",
    "# counter = 0\n",
    "# for folder_path in tqdm(all_paths, total=len(all_paths)):\n",
    "#     # get all the image names in the particular folder\n",
    "#     image_paths = os.listdir(f\"{root_dir}/{folder_path}\")\n",
    "#     # get the folder as label\n",
    "#     label = folder_path.split('.')[-1]\n",
    "#\n",
    "#     if label == 'clutter':\n",
    "#         continue\n",
    "#\n",
    "#     # save image paths in the DataFrame\n",
    "#     for image_path in image_paths:\n",
    "#         if image_path.split('.')[-1] == 'jpg':\n",
    "#             data.loc[counter, 'image_path'] = f\"{root_dir}/{folder_path}/{image_path}\"\n",
    "#             labels.append(label)\n",
    "#             counter += 1\n",
    "#\n",
    "# labels = np.array(labels)\n",
    "# # one-hot encode the labels\n",
    "# lb = LabelBinarizer()\n",
    "# labels = lb.fit_transform(labels)\n",
    "#\n",
    "# # add the image labels to the dataframe\n",
    "# for i in range(len(labels)):\n",
    "#     index = np.argmax(labels[i])\n",
    "#     data.loc[i, 'target'] = int(index)\n",
    "#\n",
    "# # shuffle the dataset\n",
    "# data = data.sample(frac=1).reset_index(drop=True)\n",
    "#\n",
    "# print(f\"Number of labels or classes: {len(lb.classes_)}\")\n",
    "# print(f\"The first one hot encoded labels: {labels[0]}\")\n",
    "# print(f\"Mapping the first one hot encoded label to its category: {lb.classes_[0]}\")\n",
    "# print(f\"Total instances: {len(data)}\")\n",
    "#\n",
    "# # save as CSV file\n",
    "# data.to_csv('data.csv', index=False)\n",
    "#\n",
    "# print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels=None, tfms=None):\n",
    "        self.X = images\n",
    "        self.y = labels\n",
    "\n",
    "        # apply augmentations\n",
    "        if tfms == 0: # if validating\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(224, 224, always_apply=True),\n",
    "                #albumentations.Normalize(mean=(140.9288, 136.0920, 129.1500), std=(81.2744, 80.3511, 83.7452), p=1.0)\n",
    "                albumentations.Normalize()\n",
    "            ])\n",
    "        else: # if training\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(224, 224, always_apply=True),\n",
    "                albumentations.HorizontalFlip(p=0.5),\n",
    "                albumentations.ShiftScaleRotate(\n",
    "                    shift_limit=0.3,\n",
    "                    scale_limit=0.3,\n",
    "                    rotate_limit=15,\n",
    "                    p=0.5\n",
    "                ),\n",
    "                albumentations.Normalize()\n",
    "                #albumentations.Normalize(mean=(140.9288, 136.0920, 129.1500), std=(81.2744, 80.3511, 83.7452), p=1.0)\n",
    "            ])\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.X[i])\n",
    "        image = image.convert('RGB')\n",
    "        image = self.aug(image=np.array(image))['image']\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        label = self.y[i]\n",
    "        return {\n",
    "            'image_X': torch.tensor(image, dtype=torch.float), \n",
    "            'image_Y': torch.tensor(image, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instances: 26802\n",
      "Validation instances: 1489\n",
      "Testing instances: 1489\n"
     ]
    }
   ],
   "source": [
    "# get the dataset ready\n",
    "df = pd.read_csv('data.csv')\n",
    "X = df.image_path.values # image paths\n",
    "y = df.target.values # targets\n",
    "batch_size = 64\n",
    "\n",
    "(xtrain, x_val_test, ytrain, y_val_test) = train_test_split(X, y,\n",
    "\ttest_size=0.10, random_state=79)\n",
    "\n",
    "(x_val, x_test, y_val, y_test) = train_test_split(x_val_test, y_val_test,\n",
    "    test_size=0.5, random_state=81)\n",
    "    \n",
    "print(f\"Training instances: {len(xtrain)}\")\n",
    "print(f\"Validation instances: {len(x_val)}\")\n",
    "print(f\"Testing instances: {len(x_test)}\")\n",
    "\n",
    "train_data = ImageDataset(xtrain, ytrain, tfms=1)\n",
    "validation_data = ImageDataset(x_val, y_val, tfms=0)\n",
    "test_data = ImageDataset(x_val, y_val, tfms=0)\n",
    "full_dataset = ImageDataset(X, y, tfms=0)\n",
    "\n",
    "# dataloaders\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "full_dataset_data_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_loader)\n",
    "# computation device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def get_mean_std(loader):\n",
    "#     # var[X] = E[X**2] - E[X]**2\n",
    "#     channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
    "#\n",
    "#     for data_index, data in enumerate(loader):\n",
    "#         X, Y = data['image_X'].to(device), data['image_Y'].to(device)\n",
    "#         channels_sum += torch.mean(X, dim=[0, 2, 3])\n",
    "#         channels_sqrd_sum += torch.mean(X ** 2, dim=[0, 2, 3])\n",
    "#         num_batches += 1\n",
    "#\n",
    "#     print('num of batches:', num_batches)\n",
    "#\n",
    "#     mean = channels_sum / num_batches\n",
    "#     std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
    "#\n",
    "#     return mean, std\n",
    "#\n",
    "#\n",
    "# train_mean, train_std = get_mean_std(train_data_loader)\n",
    "# valid_mean, valid_std = get_mean_std(valid_data_loader)\n",
    "# test_mean, test_std = get_mean_std(test_data_loader)\n",
    "#\n",
    "# print(train_mean)\n",
    "# print(train_std)\n",
    "#\n",
    "# print(valid_mean)\n",
    "# print(valid_std)\n",
    "#\n",
    "# print(test_mean)\n",
    "# print(test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_index, data in enumerate(train_data_loader):\n",
    "#     X, Y = data['image_X'].to(device), data['image_Y'].to(device)\n",
    "#     np_arr = X.detach().cpu().numpy()\n",
    "#     np_arr_2 = Y.detach().cpu().numpy()\n",
    "#     print(np.array_equal(np_arr, np_arr_2))\n",
    "#     img = Image.fromarray(np_arr[0], 'RGB')\n",
    "#     img.save('myssss.png')\n",
    "#     img.show()\n",
    "#     break\n",
    "#\n",
    "#     print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvolutionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolution encoder implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv2D_1, self.relu_1, self.maxpool_1 = self.convolve_unit(\n",
    "            in_channels_=3,\n",
    "            out_channels_=16,\n",
    "            conv_kernel_size=(3,3),\n",
    "            padding_=(1,1),\n",
    "            conv_stride=(1,1),\n",
    "            pool_kernel_size=(2,2),\n",
    "            pool_stride=(2,2)\n",
    "        )\n",
    "\n",
    "        self.conv2D_2, self.relu_2, self.maxpool_2 = self.convolve_unit(\n",
    "            in_channels_=16,\n",
    "            out_channels_=32,\n",
    "            conv_kernel_size=(3,3),\n",
    "            padding_=(1,1),\n",
    "            conv_stride=(1,1),\n",
    "            pool_kernel_size=(2,2),\n",
    "            pool_stride=(2,2)\n",
    "        )\n",
    "\n",
    "        self.conv2D_3, self.relu_3, self.maxpool_3 = self.convolve_unit(\n",
    "            in_channels_=32,\n",
    "            out_channels_=64,\n",
    "            conv_kernel_size=(3,3),\n",
    "            padding_=(1,1),\n",
    "            conv_stride=(1,1),\n",
    "            pool_kernel_size=(2,2),\n",
    "            pool_stride=(2,2)\n",
    "        )\n",
    "\n",
    "        self.conv2D_4, self.relu_4, self.maxpool_4 = self.convolve_unit(\n",
    "            in_channels_=64,\n",
    "            out_channels_=128,\n",
    "            conv_kernel_size=(3,3),\n",
    "            padding_=(1,1),\n",
    "            conv_stride=(1,1),\n",
    "            pool_kernel_size=(2,2),\n",
    "            pool_stride=(2,2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # # newly added\n",
    "        # self.linear_1 = nn.Linear(in_features=25088, out_features=1024)\n",
    "        # self.linear_batch_norm1D = nn.BatchNorm1d(num_features=1024, momentum=0.01)\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_unit( in_channels_, out_channels_, conv_kernel_size, padding_, conv_stride, pool_kernel_size, pool_stride):\n",
    "        conv2D_layer = nn.Conv2d(in_channels=in_channels_, out_channels=out_channels_, kernel_size=conv_kernel_size, padding=padding_, stride=conv_stride)\n",
    "        relu_activation = nn.ReLU(inplace=True)\n",
    "        maxpool_layer = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "\n",
    "        return conv2D_layer, relu_activation, maxpool_layer\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_batch_norm_unit( in_channels_, out_channels_, conv_kernel_size, padding_, conv_stride, pool_kernel_size, pool_stride):\n",
    "        conv2D_layer = nn.Conv2d(in_channels=in_channels_, out_channels=out_channels_, kernel_size=conv_kernel_size, padding=padding_, stride=conv_stride)\n",
    "        batch_norm_layer = nn.BatchNorm2d(num_features=out_channels_, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        relu_activation = nn.ReLU(inplace=True)\n",
    "        maxpool_layer = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "\n",
    "        return conv2D_layer,batch_norm_layer, relu_activation, maxpool_layer\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_leaky_relu_unit( in_channels_, out_channels_, conv_kernel_size, padding_, conv_stride, pool_kernel_size, pool_stride):\n",
    "        conv2D_layer = nn.Conv2d(in_channels=in_channels_, out_channels=out_channels_, kernel_size=conv_kernel_size, padding=padding_, stride=conv_stride)\n",
    "        leaky_relu_activation = nn.LeakyReLU(inplace=True)\n",
    "        maxpool_layer = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "\n",
    "        return conv2D_layer, leaky_relu_activation, maxpool_layer\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_leaky_relu_batch_norm_unit( in_channels_, out_channels_, conv_kernel_size, padding_, conv_stride, pool_kernel_size, pool_stride):\n",
    "        conv2D_layer = nn.Conv2d(in_channels=in_channels_, out_channels=out_channels_, kernel_size=conv_kernel_size, padding=padding_, stride=conv_stride)\n",
    "        batch_norm_layer = nn.BatchNorm2d(num_features=out_channels_, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        leaky_relu_activation = nn.LeakyReLU(inplace=True)\n",
    "        maxpool_layer = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "\n",
    "        return conv2D_layer, batch_norm_layer, leaky_relu_activation, maxpool_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we first downscale the image by repeated convolutions\n",
    "        x = self.conv2D_1(x) # 3, 224, 244 ---> 16, 224, 224\n",
    "        #print('After convolving:', x.size())\n",
    "        x = self.relu_1(x)\n",
    "        x = self.maxpool_1(x) # 16, 224, 224 ---> 16, 222, 222\n",
    "        #print('After maxpool:', x.size())\n",
    "\n",
    "        x = self.conv2D_2(x) # 16, 222, 222 ---> 32, 222, 222\n",
    "        #print('After convolving:', x.size())\n",
    "        x = self.relu_2(x)\n",
    "        x = self.maxpool_2(x) # 32, 222, 222 ---> 32, 220, 220\n",
    "        #print('After maxpool:', x.size())\n",
    "\n",
    "        x = self.conv2D_3(x) # 32, 219, 219 ---> 64, 220, 220\n",
    "        #print('After convolving:', x.size())\n",
    "        x = self.relu_3(x)\n",
    "        x = self.maxpool_3(x) # 64, 219, 219 ---> 64, 110, 110\n",
    "        #print('After maxpool:', x.size())\n",
    "\n",
    "        x = self.conv2D_4(x) # 64, 110, 110 ---> 128, 108, 108\n",
    "        #print('After convolving:', x.size())\n",
    "        x = self.relu_4(x)\n",
    "        x = self.maxpool_4(x) # 128, 108, 108 ---> N, 128, 14, 14\n",
    "        #print('After maxpool:', x.size())\n",
    "\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.linear_1(x)\n",
    "        # x = self.linear_batch_norm1D(x)\n",
    "        # x = self.relu(x)\n",
    "        # print(f'After linear layer:{x.size()}')\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Encoder Model converts a single Image of shape (3, 224, 224) to a feature map of shape (128, 53, 53) for the time being. This hidden representation will be forwarded to the the decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "    \"learning_rate\": 0.0016,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simple Convolution Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleConvolutionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolution decoder implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features=1024, out_features=25088)\n",
    "        self.linear_batch_norm1D = nn.BatchNorm1d(num_features=25088, momentum=0.01)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.reshape = Reshape(-1, 128, 14, 14)\n",
    "\n",
    "        self.deconv2D_1, self.relu_1 = self.deconvolve_unit(\n",
    "            in_channels_=128,\n",
    "            out_channels_=64,\n",
    "            deconv_kernel_size=(2,2),\n",
    "            stride_=(2,2),\n",
    "        )\n",
    "\n",
    "        self.deconv2D_2, self.relu_2 = self.deconvolve_unit(\n",
    "            in_channels_=64,\n",
    "            out_channels_=32,\n",
    "            deconv_kernel_size=(2,2),\n",
    "            stride_=(2,2),\n",
    "        )\n",
    "\n",
    "        self.deconv2D_3, self.relu_3 = self.deconvolve_unit(\n",
    "            in_channels_=32,\n",
    "            out_channels_=16,\n",
    "            deconv_kernel_size=(2,2),\n",
    "            stride_=(2,2),\n",
    "        )\n",
    "\n",
    "        self.deconv2D_4, self.relu_4 = self.deconvolve_unit(\n",
    "            in_channels_=16,\n",
    "            out_channels_=3,\n",
    "            deconv_kernel_size=(2,2),\n",
    "            stride_=(2,2),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def deconvolve_unit(in_channels_, out_channels_, deconv_kernel_size, stride_):\n",
    "        deconvolution_layer = nn.ConvTranspose2d(in_channels=in_channels_, out_channels=out_channels_, kernel_size=deconv_kernel_size, stride=stride_)\n",
    "\n",
    "        relu_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        return deconvolution_layer, relu_activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we first downscale the image by repeated convolutions\n",
    "        print('Before Deconvolution: ', x.size())\n",
    "        x = self.linear_1(x)\n",
    "        print('here')\n",
    "        x = self.linear_batch_norm1D(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.reshape(x)\n",
    "\n",
    "        print(f'Reshaping to:{x.size()}')\n",
    "\n",
    "        x = self.deconv2D_1(x) #\n",
    "        #print('After deconvolving:', x.size())\n",
    "        x = self.relu_1(x)\n",
    "\n",
    "        x = self.deconv2D_2(x) # 16, 222, 222 ---> 32, 222, 222\n",
    "        #print('After deconvolving:', x.size())\n",
    "        x = self.relu_2(x)\n",
    "\n",
    "        x = self.deconv2D_3(x) # 32, 219, 219 ---> 64, 220, 220\n",
    "        #print('After deconvolving:', x.size())\n",
    "        x = self.relu_3(x)\n",
    "\n",
    "        x = self.deconv2D_4(x) # 64, 110, 110 ---> 128, 108, 108\n",
    "        #print('After deconvolving:', x.size())\n",
    "        x = self.relu_4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "class LRScheduler:\n",
    "    \"\"\"\n",
    "    Learning rate scheduler. If the validation loss does not decrease for the\n",
    "    given number of `patience` epochs, then the learning rate will decrease by\n",
    "    by given `factor`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, optimizer, patience=5, min_lr=1e-6, factor=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        new_lr = old_lr * factor\n",
    "        :param optimizer: the optimizer we are using\n",
    "        :param patience: how many epochs to wait before updating the lr\n",
    "        :param min_lr: least lr value to reduce to while updating\n",
    "        :param factor: factor by which the lr should be updated\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            patience=self.patience,\n",
    "            factor=self.factor,\n",
    "            min_lr=self.min_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "    def __call__(self, val_loss):\n",
    "        self.lr_scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Encoder Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
    "        print(self.maxpool1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.maxpool4 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.maxpool5 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downscale the image with conv maxpool etc.\n",
    "        x = self.conv1(x)\n",
    "        print('After convolving:', x.size())\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        print('After maxpool:', x.size())\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        print('After convolving:', x.size())\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        print('After maxpool:', x.size())\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        print('After convolving:', x.size())\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        print('After maxpool:', x.size())\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        print('After convolving:', x.size())\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        print('After maxpool:', x.size())\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Decoder Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, (2, 2), stride=(2, 2))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        print(self.deconv1)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv5 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Upscale the image with convtranspose etc.\n",
    "        x = self.deconv1(x)\n",
    "        print('After deconvolving:', x.size())\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        print('After deconvolving:', x.size())\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        print('After deconvolving:', x.size())\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        print('After deconvolving:', x.size())\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "        print('After deconvolving:', x.size())\n",
    "        x = self.relu5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResnetEncoder:\n",
    "    def __init__(self):\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        print(resnet.fc.in_features)\n",
    "\n",
    "        self.fc1 = nn.Linear(resnet.fc.in_features, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(self.fc_hidden1, momentum=0.01)\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.fc_hidden2, momentum=0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "resnet = models.resnet152(pretrained=True)\n",
    "modules = list(resnet.children())[:-1]\n",
    "resnet_ = nn.Sequential(*modules)\n",
    "print(resnet.fc.in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(encoder, decoder, train_data_loader, loss_function, optimizer, device):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #  Set networks to train mode.\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for batch_idx, image in enumerate(train_data_loader):\n",
    "        # Move images to device\n",
    "        train_img = image['image_X'].to(device)\n",
    "        target_img = image['image_Y'].to(device)\n",
    "\n",
    "        # Zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed the train images to encoder\n",
    "        encoder_output = encoder(train_img)\n",
    "        decoder_output = decoder(encoder_output)\n",
    "\n",
    "        # Decoder output is reconstructed image\n",
    "        loss = loss_function(decoder_output, target_img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Return the loss\n",
    "    return loss.item()\n",
    "\n",
    "def val_step(encoder, decoder, valid_data_loader, loss_function, device):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Set to eval mode.\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, img in enumerate(valid_data_loader):\n",
    "\n",
    "            # Move to device\n",
    "            train_img = img['image_X'].to(device)\n",
    "            target_img = img['image_Y'].to(device)\n",
    "\n",
    "            # Feed the train images to encoder\n",
    "            encoder_output = encoder(train_img)\n",
    "            decoder_output = decoder(encoder_output)\n",
    "\n",
    "            # Decoder output is reconstructed image\n",
    "\n",
    "            # Validation loss for encoder and decoder.\n",
    "            loss = loss_function(decoder_output, target_img)\n",
    "    # Return the loss\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # We use Mean squared loss which computes difference between two images.\n",
    "\n",
    "encoder = SimpleConvolutionEncoder() # encoder model\n",
    "decoder = SimpleConvolutionDecoder() # decoder model\n",
    "\n",
    "# Load the state dict of encoder\n",
    "encoder.load_state_dict(torch.load('./encoder_model_.pt', map_location=device))\n",
    "# encoder.to(device)\n",
    "\n",
    "device = \"cuda\"  # GPU device\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "encoder_params = list(encoder.parameters())\n",
    "decoder_params = list(decoder.parameters())\n",
    "autoencoder_params = encoder_params + decoder_params\n",
    "optimizer = torch.optim.Adam(autoencoder_params, lr=0.002) # Adam Optimizer\n",
    "lr_scheduler = LRScheduler(optimizer=optimizer)\n",
    "\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Deconvolution:  torch.Size([64, 128, 14, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (114688x14 and 1024x25088)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59384/2283816447.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epochs = {epoch}, Training Loss : {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_59384/1120842373.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(encoder, decoder, train_data_loader, loss_function, optimizer, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Feed the train images to encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Decoder output is reconstructed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_59384/145314014.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# we first downscale the image by repeated convolutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Before Deconvolution: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'here'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_batch_norm1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (114688x14 and 1024x25088)"
     ]
    }
   ],
   "source": [
    "# Usual Training Loop\n",
    "max_loss = 1e10\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "    train_loss = train_step(encoder, decoder, train_data_loader, criterion, optimizer, device=device)\n",
    "\n",
    "    print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n",
    "    #wandb.log({\"\\ntrain_loss\": train_loss, \"epoch\": epoch})\n",
    "\n",
    "    val_loss = val_step(encoder, decoder, valid_data_loader, criterion, device=device)\n",
    "\n",
    "    print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n",
    "    #wandb.log({\"\\nvalid_loss\": val_loss, \"epoch\": epoch})\n",
    "\n",
    "    # Simple Best Model saving\n",
    "    if val_loss < max_loss:\n",
    "        # print(\"\\nValidation Loss decreased, saving new best model\")\n",
    "        print(\"\\nValidation Loss decreased\")\n",
    "        # torch.save(encoder.state_dict(), \"encoder_model_.pt\")\n",
    "        # torch.save(decoder.state_dict(), \"decoder_model_.pt\")\n",
    "        max_loss = val_loss\n",
    "\n",
    "    # updating learning rate with validation loss\n",
    "    lr_scheduler(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding(encoder, full_loader, embedding_dim, device):\n",
    "    \"\"\"\n",
    "    Creates embedding using encoder from dataloader.\n",
    "    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder\n",
    "    full_loader: PyTorch dataloader, containing (images, images) over entire dataset.\n",
    "    embedding_dim: Tuple (c, h, w) Dimension of embedding = output of encoder dimensions.\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    Returns: Embedding of size (num_images_in_loader + 1, c, h, w)\n",
    "    \"\"\"\n",
    "    # Set encoder to eval mode.\n",
    "    encoder.eval()\n",
    "    # Just a place holder for our 0th image embedding.\n",
    "    embedding = torch.randn(embedding_dim)\n",
    "\n",
    "    # Again we do not compute loss here so. No gradients.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, image in enumerate(full_loader):\n",
    "\n",
    "            # Move images to device\n",
    "            img = image['image_X'].to(device)\n",
    "\n",
    "            # Get encoder outputs and move outputs to cpu\n",
    "            enc_output = encoder(img).cpu()\n",
    "            # Keep adding these outputs to embeddings.\n",
    "            embedding = torch.cat((embedding, enc_output), 0)\n",
    "\n",
    "    # Return the embeddings\n",
    "    print(f'The Embedding Shape: {embedding.size()}')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62291/4068605909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# We need feature representations for complete dataset not just train and validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Hence we use full loader here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_dataset_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Convert embedding to numpy and save them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_62291/2624752403.py\u001b[0m in \u001b[0;36mcreate_embedding\u001b[0;34m(encoder, full_loader, embedding_dim, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Again we do not compute loss here so. No gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Move images to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_62291/3029885339.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \"\"\"\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save the feature representations.\n",
    "EMBEDDING_SHAPE = (1, 128, 14, 14) # This we know from our encoder\n",
    "\n",
    "# We need feature representations for complete dataset not just train and validation.\n",
    "# Hence we use full loader here.\n",
    "embedding = create_embedding(encoder, full_dataset_data_loader, EMBEDDING_SHAPE, device)\n",
    "\n",
    "# Convert embedding to numpy and save them\n",
    "numpy_embedding = embedding.cpu().detach().numpy()\n",
    "num_images = numpy_embedding.shape[0]\n",
    "\n",
    "# Save the embeddings for complete dataset, not just train\n",
    "flattened_embedding = numpy_embedding.reshape((num_images, -1))\n",
    "np.save(\"data_embedding.npy\", flattened_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_query_images(image):\n",
    "    \"\"\"\n",
    "    Preprocess an Image and convert it to an image Tensor\n",
    "    :param image:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image = Image.open(image)\n",
    "    image_tensor = T.ToTensor()(image)\n",
    "    image_tensor = T.Resize((224, 224))(image_tensor)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    print(image_tensor.size())\n",
    "\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_similar_images(image, num_images, embedding, device):\n",
    "    \"\"\"\n",
    "    Given an image and number of similar images to search.\n",
    "    Returns the num_images closest neares images.\n",
    "    Args:\n",
    "    image: Image whose similar images are to be found.\n",
    "    num_images: Number of similar images to find.\n",
    "    embedding : A (num_images, embedding_dim) Embedding of images learnt from auto-encoder.\n",
    "    device : \"cuda\" or \"cpu\" device.\n",
    "    \"\"\"\n",
    "    image_tensor = preprocess_query_images(image=image)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(type(image_tensor))\n",
    "        print(image_tensor.size())\n",
    "        image_embedding = encoder(image_tensor)\n",
    "        image_embedding = image_embedding.cpu().detach().numpy()\n",
    "\n",
    "    print(f'Embedding Computation Done!, The Embedding Size: {image_embedding.shape}')\n",
    "\n",
    "    embedding_flattened = image_embedding.reshape((image_embedding.shape[0], -1))\n",
    "\n",
    "    print(f'Embedding Flattening Done!. Shape is: {embedding_flattened.shape}')\n",
    "\n",
    "    print('Computing Nearest Neighbors!')\n",
    "    knn = NearestNeighbors(n_neighbors=num_images, metric=\"cosine\")\n",
    "    knn.fit(embedding)\n",
    "\n",
    "\n",
    "    print('Computation Done! Finding Nearest Neighbors!')\n",
    "    _, indices = knn.kneighbors(embedding_flattened)\n",
    "    indices_list = indices.tolist()\n",
    "\n",
    "    print(indices_list)\n",
    "    return indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_similar_images(indices_list):\n",
    "    \"\"\"\n",
    "    Plots images that are similar to indices obtained from computing simliar images.\n",
    "    Args:\n",
    "    indices_list : List of List of indexes. E.g. [[1, 2, 3]]\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv('./data.csv')\n",
    "    \n",
    "    indices = indices_list[0]\n",
    "    for index in indices:\n",
    "        if index == 0:\n",
    "            # index 0 is a dummy embedding.\n",
    "            pass\n",
    "        else:\n",
    "            #img_name = str(index - 1) + \".jpg\"\n",
    "            img_path = df.iloc[index-1, 0]\n",
    "            # print(img_path)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            img.save(f\"../outputs/query_image_3/recommended_{index - 1}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.846308    0.5046669  -1.2059591  ... -1.1469277  -1.0656835\n",
      "   1.1932936 ]\n",
      " [ 2.646197    0.8255824   1.4949089  ...  0.          0.\n",
      "   0.03218506]\n",
      " [ 2.8518581   4.0531783   3.9981594  ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 3.7282143   3.6727903   3.2174745  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.1175067   3.4398472   3.4398472  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.3046029  10.425444    4.0060315  ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    " # Load the state dict of encoder\n",
    "#encoder.load_state_dict(torch.load('./encoder_model_.pt', map_location=device))\n",
    "encoder.eval()\n",
    "encoder.to(device)\n",
    "\n",
    "# Loads the embedding\n",
    "embedding = np.load('./data_embedding.npy')\n",
    "\n",
    "print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25088,)\n"
     ]
    }
   ],
   "source": [
    "print(embedding[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 224, 224])\n",
      "Embedding Computation Done!, The Embedding Size: (1, 128, 14, 14)\n",
      "Embedding Flattening Done!. Shape is: (1, 25088)\n",
      "Computing Nearest Neighbors!\n",
      "Computation Done! Finding Nearest Neighbors!\n",
      "[[3207, 19603, 29568, 25823, 1735]]\n"
     ]
    }
   ],
   "source": [
    "indices_list = compute_similar_images(\n",
    "    '/home/akil/Work/Work/AI/projects/deep-learning-projects/image-similarity-search/data/caltech256/256_ObjectCategories/001.ak47/001_0001.jpg', 5, embedding, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/caltech256/256_ObjectCategories/183.sextant/183_0082.jpg\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62291/1992699375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_similar_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_62291/765523513.py\u001b[0m in \u001b[0;36mplot_similar_images\u001b[0;34m(indices_list)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m# print(img_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_similar_images(indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50ac257f6f2ab5f1510b89f2fdfed253ce2cff535875d295f2a6522bbbe1deb8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
